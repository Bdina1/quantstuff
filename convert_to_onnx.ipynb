{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory and prevent fragmentation issues in one cell\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Perform garbage collection to free up unreferenced memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"GPU memory cache cleared and memory fragmentation settings applied.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will:\n",
    "\n",
    "1. Set the environment variable to help prevent memory fragmentation. \n",
    "2. Clear the PyTorch CUDA cache. \n",
    "3. Run garbage collection to free up additional memory. \n",
    "\n",
    "You can run this cell anytime you need to clear GPU memory before running your model export or inference tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Process to Export and Save the ONNX Model with External Data to a Specified Folder\n",
    "### Step 1: Create Output Directory\n",
    "Define the output directory where all generated files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the directory to save ONNX and related files\n",
    "output_dir = \"model_export\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Model and Prepare for Export\n",
    "Here, you’ll load the model and make any required configurations, such as disabling FlashAttention and converting to FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deca1/anaconda3/envs/quant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.72s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Example inference\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_device_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Send to the first device in map\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m model(dummy_input)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model with device_map=\"auto\" for automatic model parallelism\n",
    "model_name = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\"  # Automatically distribute layers across GPUs\n",
    ")\n",
    "\n",
    "# Convert model to half-precision (FP16)\n",
    "model = model.half()\n",
    "\n",
    "# Example inference\n",
    "dummy_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\").input_ids.to(model.hf_device_map[0])  # Send to the first device in map\n",
    "output = model(dummy_input)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Export the Model to ONNX with External Data to the Output Directory\n",
    "The ONNX export will automatically generate external data files if the model is large. Specify the path for the main ONNX file in the output_dir so all related files are saved in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the ONNX file in the output directory\n",
    "onnx_model_path = os.path.join(output_dir, \"phi_3.5_vision.onnx\")\n",
    "\n",
    "# Export to ONNX format with potential for external data files\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence\"}},\n",
    "    opset_version=13,\n",
    "    export_params=True,  # Saves parameters in the model file if possible\n",
    "    keep_initializers_as_inputs=True  # Keeps initializers within the ONNX model graph\n",
    ")\n",
    "\n",
    "print(f\"Model exported to ONNX format in {output_dir} successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Quantize the ONNX Model to INT8 and Save to the Same Folder\n",
    "Using ONNX Runtime, quantize the model to INT8 precision. Save the quantized model in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Define the path for the quantized ONNX model\n",
    "quantized_model_path = os.path.join(output_dir, \"phi_3.5_vision_quantized.onnx\")\n",
    "\n",
    "# Quantize to INT8\n",
    "quantize_dynamic(\n",
    "    onnx_model_path,\n",
    "    quantized_model_path,\n",
    "    op_types_to_quantize=[\"MatMul\"],\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(f\"Quantized model saved to {quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Convert the Quantized ONNX Model to TensorRT and Save to the Output Directory\n",
    "For TensorRT conversion, use trtexec to optimize and convert the quantized ONNX model to a TensorRT engine. This step requires running a shell command, so here’s how to do it in Jupyter or a Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the TensorRT engine file\n",
    "tensorrt_model_path = os.path.join(output_dir, \"phi_3.5_vision_quantized.trt\")\n",
    "\n",
    "# Run trtexec to convert the model to TensorRT\n",
    "!trtexec --onnx={quantized_model_path} --saveEngine={tensorrt_model_path} --int8\n",
    "\n",
    "print(f\"TensorRT engine file saved to {tensorrt_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Verify All Files Are in the Specified Output Directory\n",
    "After running the notebook, the model_export directory should contain:\n",
    "\n",
    "phi_3.5_vision.onnx: The exported ONNX model.\n",
    "phi_3.5_vision.onnx.data_0 (and other .data files, if applicable): External data files for model weights and biases.\n",
    "phi_3.5_vision_quantized.onnx: The INT8 quantized ONNX model.\n",
    "phi_3.5_vision_quantized.trt: The TensorRT engine file for deployment on NVIDIA devices.\n",
    "### Step 7: Transfer and Run on Jetson Orin\n",
    "Refer to the instructions for transferring and running the TensorRT model on Jetson Orin provided in the previous messages. Ensure the phi_3.5_vision_quantized.trt file is transferred to the Jetson device, as it contains the fully optimized and quantized model for efficient inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "**Export Directory**: All files are saved in the `model_export` directory.  \n",
    "**ONNX Export**: The original ONNX model and external data files are saved.  \n",
    "**Quantization**: The INT8 model is saved in the same directory.  \n",
    "**TensorRT Engine**: The final optimized TensorRT engine file is saved for deployment on Jetson Orin.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
